{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 风格迁移之ReCoNet\n",
    "\n",
    "风格迁移是通过算法将图像从原始图像风格迁移到目标图像的风格，同时保持原图像的内容。如果要对视频做迁移，还需要保持前后帧的内容不产生跳变，这样产生的风格迁移视频会更流畅。ReCoNet是一个轻量的图像风格迁移模型，可以快速迁移图像风格，因此可以支持实时视频风格迁移。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 模型简介\n",
    "\n",
    "在模型设计上，ReCoNet由一个encoder-decoder结构组成，encoder包含3层conv2d和4层残差连接的conv2d组成，decoder是3层conv2d。使用一个vgg16预训练模型进行图像内容encode。整个模型层数较少，因此可以实现很快的推理速度。在训练时，也比较快速，仅需要8小时就可以实现32000steps。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 数据处理\n",
    "\n",
    "开始实验之前，请确保本地已经安装了Python环境并安装了MindSpore和MindSpore Vision套件。\n",
    "\n",
    "### 数据准备\n",
    "\n",
    "本案例使用Sceneflow数据集中的Monkaa和Flyingthings3d作为训练集。请在数据集官网<https://lmb.informatik.uni-freiburg.de/resources/datasets/SceneFlowDatasets.en.html>下载Monkaa和Flyingthings3d数据集的RGB images(finalpass), Optical flow, Motion boundaries三个部分，共6个压缩包。如图红框所示：\n",
    "\n",
    "![show_images](images/dataset.png)\n",
    "\n",
    "请将解压后的数据集按如下文件目录放置：\n",
    "\n",
    "```text\n",
    "\n",
    ".datasets/\n",
    "    └── Monkaa\n",
    "         ├── frames_finalpass\n",
    "         ├── motion_boundaries\n",
    "         └── optical_flow\n",
    "    └── Flyingthings3d\n",
    "         ├── frames_finalpass\n",
    "         ├── motion_boundaries\n",
    "         └── optical_flow\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "is_executing": true,
     "name": "#%% md\n"
    }
   },
   "source": [
    "### 数据加载\n",
    "\n",
    "通过数据集加载接口加载数据集，并通过transform变换备输入模型使用。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mindspore.dataset as ds\n",
    "\n",
    "from src.dataset.dataset import Monkaa, Flyingthings3d\n",
    "\n",
    "import sys\n",
    "sys.path.append('./src')\n",
    "\n",
    "COLUMNS = ['frame', 'pre_frame', 'optical_flow', 'reverse_optical_flow', 'motion_boundaries', 'index']\n",
    "\n",
    "monkaa_path = './dataset/Monkaa'\n",
    "ft3d_path = './dataset/Flyingthings3d'\n",
    "\n",
    "monkaa_dataset = ds.GeneratorDataset(Monkaa(monkaa_path), COLUMNS)\n",
    "ft3d_dataset = ds.GeneratorDataset(Flyingthings3d(ft3d_path), COLUMNS)\n",
    "\n",
    "train_dataset = monkaa_dataset + ft3d_dataset\n",
    "train_dataset = train_dataset.batch(batch_size=2)\n",
    "\n",
    "print(\"=========Complete data loading===========\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 构建网络\n",
    "\n",
    "![reconet_structure](./images/reconet.png)\n",
    "\n",
    "ReCoNet由一个encoder-decoder结构组成，encoder包含3层conv2d和4层残差连接的conv2d组成，decoder是3层conv2d。使用一个vgg16预训练模型进行图像内容encode。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import mindspore.nn as nn\n",
    "\n",
    "class ConvLayer(nn.Cell):\n",
    "    \"\"\"\n",
    "    Conv2d layer\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, in_channels, out_channels, kernel_size, stride):\n",
    "        super().__init__()\n",
    "        self.kernel_size = kernel_size\n",
    "        self.conv2d = nn.Conv2d(in_channels,\n",
    "                                out_channels,\n",
    "                                kernel_size,\n",
    "                                stride,\n",
    "                                has_bias=True,\n",
    "                                pad_mode='valid')\n",
    "\n",
    "    def construct(self, x):\n",
    "        \"\"\"Construct ConvLayer.\"\"\"\n",
    "        x = mindspore.numpy.pad(x,\n",
    "                                (\n",
    "                                    (0, 0),\n",
    "                                    (0, 0),\n",
    "                                    (self.kernel_size // 2, self.kernel_size // 2),\n",
    "                                    (self.kernel_size // 2, self.kernel_size // 2)\n",
    "                                ),\n",
    "                                mode='reflect')\n",
    "        x = self.conv2d(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class ConvNormLayer(nn.Cell):\n",
    "    \"\"\"\n",
    "    Conv2d with InstanceNorm\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, in_channels, out_channels, kernel_size, stride, activation=True):\n",
    "        super().__init__()\n",
    "\n",
    "        layers = [\n",
    "            ConvLayer(in_channels, out_channels, kernel_size, stride),\n",
    "            nn.InstanceNorm2d(out_channels, affine=True)\n",
    "        ]\n",
    "        if activation:\n",
    "            layers.append(nn.ReLU())\n",
    "\n",
    "        self.layers = nn.SequentialCell(layers)\n",
    "\n",
    "    def construct(self, x):\n",
    "        \"\"\"Construct ConvNormLayer.\"\"\"\n",
    "        x = self.layers(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class ResLayer(nn.Cell):\n",
    "    \"\"\"\n",
    "    ReCoNet res layer\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, in_channels, out_channels, kernel_size):\n",
    "        super().__init__()\n",
    "        self.branch = nn.SequentialCell(\n",
    "            [\n",
    "                ConvNormLayer(in_channels, out_channels, kernel_size, 1),\n",
    "                ConvNormLayer(out_channels, out_channels, kernel_size, 1, activation=False)\n",
    "            ]\n",
    "        )\n",
    "\n",
    "        self.activation = nn.ReLU()\n",
    "\n",
    "    def construct(self, x):\n",
    "        \"\"\"Construct ResLayer.\"\"\"\n",
    "        x = x + self.branch(x)\n",
    "        x = self.activation(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class ConvTanhLayer(nn.Cell):\n",
    "    \"\"\"\n",
    "    Conv2d with tanh activation function\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, in_channels, out_channels, kernel_size, stride):\n",
    "        super().__init__()\n",
    "        self.layers = nn.SequentialCell(\n",
    "            [\n",
    "                ConvLayer(in_channels, out_channels, kernel_size, stride),\n",
    "                nn.Tanh()\n",
    "            ]\n",
    "        )\n",
    "\n",
    "    def construct(self, x):\n",
    "        \"\"\"Construct ConvTanhLayer.\"\"\"\n",
    "        x = self.layers(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class Encoder(nn.Cell):\n",
    "    \"\"\"\n",
    "    ReCoNet Encoder layer\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.layers = nn.SequentialCell(\n",
    "            [\n",
    "                ConvNormLayer(3, 48, 9, 1),\n",
    "                ConvNormLayer(48, 96, 3, 2),\n",
    "                ConvNormLayer(96, 192, 3, 2),\n",
    "                ResLayer(192, 192, 3),\n",
    "                ResLayer(192, 192, 3),\n",
    "                ResLayer(192, 192, 3),\n",
    "                ResLayer(192, 192, 3)\n",
    "            ]\n",
    "        )\n",
    "\n",
    "    def construct(self, x):\n",
    "        \"\"\"Construct Encoder.\"\"\"\n",
    "        x = self.layers(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class Decoder(nn.Cell):\n",
    "    \"\"\"\n",
    "    ReCoNet decoder layer\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.up_sample = nn.ResizeBilinear()\n",
    "        self.conv1 = ConvNormLayer(192, 96, 3, 1)\n",
    "        self.conv2 = ConvNormLayer(96, 48, 3, 1)\n",
    "        self.conv3 = ConvTanhLayer(48, 3, 9, 1)\n",
    "\n",
    "    def construct(self, x):\n",
    "        \"\"\"Construct Decoder.\"\"\"\n",
    "        x = self.up_sample(x, scale_factor=2)\n",
    "        x = self.conv1(x)\n",
    "        x = self.up_sample(x, scale_factor=2)\n",
    "        x = self.conv2(x)\n",
    "        x = self.conv3(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class ReCoNet(nn.Cell):\n",
    "    \"\"\"\n",
    "    ReCoNet model\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.encoder = Encoder()\n",
    "        self.decoder = Decoder()\n",
    "\n",
    "    def construct(self, x):\n",
    "        \"\"\"Construct ReCoNet.\"\"\"\n",
    "        x = self.encoder(x)\n",
    "        x = self.decoder(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## VGG16\n",
    "\n",
    "VGG16是一个预训练模型，可以实现图像encoding。本案例可以从<https://download.mindspore.cn/vision/reconet/vgg16_for_reconet.ckpt>下载我们提供的VGG16预训练，也可以从model zoo或其他地方下载VGG16预训练模型，下载后请在reconet根目录下创建model文件夹将vgg模型存放在目录中。使用我们提供的VGG16预训练模型可以使用默认的训练参数，若使用其他VGG16预训练模型需要自行调整训练参数，具体调参建议可以参考readme.md中VGG16 pretrain model部分。\n",
    "\n",
    "在ReCoNet中，VGG16 encoder使用3, 8, 15, 22层的输出来获取不同程度的图像内容encoding，可以从多个维度表征图像特征。在训练过程中，VGG模型不需要参与梯度下降，仅作为一个图像特征encoder使用。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "cfg = {\n",
    "    '11': [64, 'M', 128, 'M', 256, 256, 'M', 512, 512, 'M', 512, 512, 'M'],\n",
    "    '13': [64, 64, 'M', 128, 128, 'M', 256, 256, 'M', 512, 512, 'M', 512, 512, 'M'],\n",
    "    '16': [64, 64, 'M', 128, 128, 'M', 256, 256, 256, 'M', 512, 512, 512, 'M', 512, 512, 512, 'M'],\n",
    "    '19': [64, 64, 'M', 128, 128, 'M', 256, 256, 256, 256, 'M', 512, 512, 512, 512, 'M', 512, 512, 512, 512, 'M'],\n",
    "}\n",
    "\n",
    "\n",
    "def _make_layer(base, padding=0, pad_mode='same', has_bias=False, batch_norm=False):\n",
    "    \"\"\"Make stage network of VGG.\n",
    "\n",
    "    Args:\n",
    "        base (list): Configuration for different layers, mainly the channel number of Conv layer.\n",
    "        padding (int): Conv2d padding value. Default: 0.\n",
    "        pad_mode (str): Conv2d pad mode. Default: False.\n",
    "        has_bias (int): Whether conv2d has bias\n",
    "        batch_norm(bool): Whether vgg has batch norm layer\n",
    "\n",
    "    Returns:\n",
    "        Vgg layers\n",
    "    \"\"\"\n",
    "    layers = []\n",
    "    in_channels = 3\n",
    "    for v in base:\n",
    "        if v == 'M':\n",
    "            layers += [nn.MaxPool2d(kernel_size=2, stride=2)]\n",
    "        else:\n",
    "            conv2d = nn.Conv2d(in_channels=in_channels,\n",
    "                               out_channels=v,\n",
    "                               kernel_size=3,\n",
    "                               padding=padding,\n",
    "                               pad_mode=pad_mode,\n",
    "                               has_bias=has_bias)\n",
    "\n",
    "            if batch_norm:\n",
    "                layers += [conv2d, nn.BatchNorm2d(v), nn.ReLU()]\n",
    "            else:\n",
    "                layers += [conv2d, nn.ReLU()]\n",
    "            in_channels = v\n",
    "    return nn.SequentialCell(layers)\n",
    "\n",
    "\n",
    "class Vgg(nn.Cell):\n",
    "    \"\"\"\n",
    "    VGG network definition.\n",
    "\n",
    "    Args:\n",
    "        base (list): Configuration for different layers, mainly the channel number of Conv layer.\n",
    "        padding (int): Padding value. Default: 0.\n",
    "        pad_mode (str): Pad mode. Default: False.\n",
    "        has_bias (int): Whether conv2d has bias\n",
    "        batch_norm(bool): Whether vgg has batch norm layer\n",
    "\n",
    "    Returns:\n",
    "        Tensor, infer output tensor.\n",
    "\n",
    "    Examples:\n",
    "        >>> Vgg('16')\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self,\n",
    "                 edition,\n",
    "                 padding=1,\n",
    "                 pad_mode='pad',\n",
    "                 has_bias=False,\n",
    "                 batch_norm=False):\n",
    "        super(Vgg, self).__init__()\n",
    "        self.layers = _make_layer(cfg[edition], padding, pad_mode, has_bias, batch_norm)\n",
    "        self.flatten = nn.Flatten()\n",
    "\n",
    "    def construct(self, x):\n",
    "        x = self.layers(x)\n",
    "        x = self.flatten(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class VggEncoder(Vgg):\n",
    "    \"\"\"\n",
    "    Encode style features with VGG network.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, edition='16', padding=0, pad_mode='same', has_bias=True):\n",
    "        super(VggEncoder, self).__init__(edition=edition,\n",
    "                                         padding=padding,\n",
    "                                         pad_mode=pad_mode,\n",
    "                                         has_bias=has_bias)\n",
    "\n",
    "    def encode(self, x):\n",
    "        \"\"\"\n",
    "        Get encoded style features from specific vgg layers\n",
    "        For reconet, [3, 8, 15, 22] is used\n",
    "        \"\"\"\n",
    "        layers_of_interest = [3, 8, 15, 22]\n",
    "        result = []\n",
    "        for i, layer in enumerate(self.layers):\n",
    "            x = layer(x)\n",
    "            if i in layers_of_interest:\n",
    "                result.append(x)\n",
    "        return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### 损失函数\n",
    "\n",
    "ReCoNet的损失设计十分明了，为了保证风格迁移后的图像内容与原图像一致，需要让原图像通过VGG16得到的encoded feature与生成图像的encoded feature尽可能接近，因此对两个特征计算L2 loss：\n",
    "\n",
    "$$\n",
    "\\mathcal{L_{content-loss}} := \\sum_{n=1}^{N}\\lVert x_{input}-x_{output}\\rVert^2\n",
    "\\tag{1}\n",
    "$$\n",
    "\n",
    "对于风格损失，与content loss一样，只需要将输出的输出图像的style encoding与风格图像的style encoding尽可能接近，因此也对两个特征计算L2 Loss，注意风格encoding使用了4个层的输出矩阵所以还需要对四层的style loss求和：\n",
    "\n",
    "$$\n",
    "\\mathcal{L_{style-loss}} := \\sum_{m=1}^{4} \\sum_{n=1}^{N}\\lVert x_{output}-x_{style}\\rVert^2\n",
    "\\tag{2}\n",
    "$$\n",
    "\n",
    "为了保证时序性，ReCoNet共设计了两种feature temporal loss和output temporal loss，其中feature temporal loss使用当前帧输入图像的encoding与前一帧输入图像的encoding一同计算L2 loss：\n",
    "\n",
    "$$\n",
    "\\mathcal{L_{style-loss}} := \\sum_{m=1}^{4} \\sum_{n=1}^{N}\\lVert i_{input}-o_{input}\\rVert^2\n",
    "\\tag{3}\n",
    "$$\n",
    "\n",
    "而output temporal loss，则是将当前帧输入图像与前一帧输入图像的特征差值与当前帧decode后的输出与前一帧图像的输出的差值，计算L2 loss，这样可以让输出的图像的前后两帧尽可能保持与输入相似的差异：\n",
    "\n",
    "$$\n",
    "\\mathcal{L_{style-loss}} := \\sum_{n=1}^{N}\\lVert (o_{input}-o_{preframe}) - (i_{input} - i_{preframe})\\rVert^2\n",
    "\\tag{4}\n",
    "$$\n",
    "\n",
    "全变差正则化损失（Total_Variation_Loss）的定义和实现参考[2]。全变差正则化损失能够使得输出图像空间更平滑，具体定义见代码。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import mindspore.ops as ops\n",
    "\n",
    "from src.utils.reconet_utils import occlusion_mask_from_flow, preprocess_for_vgg, gram_matrix, \\\n",
    "    resize_optical_flow, warp_optical_flow, rgb_to_luminance\n",
    "\n",
    "# 由于total variation loss是一个自定义loss需要自行定义\n",
    "class Total_Variation_Loss(nn.Cell):\n",
    "    \"\"\"Total variation loss\"\"\"\n",
    "    def __init__(self, reduction='sum'):\n",
    "        super(Total_Variation_Loss, self).__init__()\n",
    "        self.reduction = reduction\n",
    "        self.abs = ops.Abs()\n",
    "\n",
    "    def construct(self, y):\n",
    "        \"\"\"Total variation loss\"\"\"\n",
    "        return mindspore.numpy.sum(self.abs(y[:, :, :, :-1] - y[:, :, :, 1:])) + \\\n",
    "               mindspore.numpy.sum(self.abs(y[:, :, :-1, :] - y[:, :, 1:, :]))\n",
    "\n",
    "# Content loss， style loss， feature temporal loss， output temporal loss使用mindspore内置的MSELoss，并在Cell_with_loss中实现\n",
    "# 此处仅展示loss的实现方法\n",
    "def content_loss(self, output_feature, input_feature):\n",
    "    \"\"\"Content loss\"\"\"\n",
    "    _, c, h, w = output_feature.shape\n",
    "    return self.l2_loss(output_feature, input_feature) / (c * h * w)\n",
    "\n",
    "def style_loss(self, vgg_feature_gram, style_gram):\n",
    "    \"\"\"Style loss\"\"\"\n",
    "    loss = 0\n",
    "    for content_fm, style_gm in zip(vgg_feature_gram, style_gram):\n",
    "        loss += self.l2_loss(gram_matrix(content_fm), style_gm)\n",
    "    return loss\n",
    "\n",
    "def feature_temporal_loss(self, feature_maps, previous_feature_maps, reverse_optical_flow, occlusion_mask):\n",
    "    \"\"\"Feature temporal loss\"\"\"\n",
    "    _, c, h, w = feature_maps.shape\n",
    "\n",
    "    reverse_optical_flow_resized = resize_optical_flow(reverse_optical_flow, h, w)\n",
    "    occlusion_mask_resized = ops.ResizeNearestNeighbor((h, w))(occlusion_mask)\n",
    "    feature_maps = occlusion_mask_resized * feature_maps\n",
    "    pre_feature_maps = occlusion_mask_resized * warp_optical_flow(previous_feature_maps, reverse_optical_flow_resized)\n",
    "    loss = self.l2_loss(feature_maps, pre_feature_maps) / (c * h * w)\n",
    "    return loss\n",
    "\n",
    "def output_temporal_loss(self, input_frame, previous_input_frame, output_frame, previous_output_frame,\n",
    "                         reverse_optical_flow, occlusion_mask):\n",
    "    \"\"\"Output temporal loss\"\"\"\n",
    "    input_diff = input_frame - warp_optical_flow(previous_input_frame, reverse_optical_flow)\n",
    "    output_diff = output_frame - warp_optical_flow(previous_output_frame, reverse_optical_flow)\n",
    "    luminance_input_diff = rgb_to_luminance(input_diff)\n",
    "    luminance_input_diff = ops.ExpandDims()(luminance_input_diff, 1)\n",
    "    _, _, h, w = input_frame.shape\n",
    "    loss = self.l2_loss(occlusion_mask * output_diff, occlusion_mask * luminance_input_diff) / (h * w)\n",
    "    return loss\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 模型实现\n",
    "\n",
    "由于ReCoNet损失需要使用模型中间输出，并且较为复杂，无法简单通过传入loss_fn的方式来封装训练模型，因此需要再实现一个ReCoNet_with_loss模块，初始化时将ReCoNet模型与VGG模型传入模块中，并将训练和loss计算的过程在construct中实现。在训练时，当输入sample时，模型可以返回loss，这样才可以在mindspore框架下实现训练。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RecoNet_with_Loss(nn.Cell):\n",
    "    def __init__(self,\n",
    "                 model: ReCoNet,\n",
    "                 vgg,\n",
    "                 alpha,\n",
    "                 beta,\n",
    "                 gamma,\n",
    "                 lambda_f,\n",
    "                 lambda_o):\n",
    "        super(RecoNet_with_Loss, self).__init__(auto_prefix=False)\n",
    "        self.backbone = model\n",
    "        self.net = vgg\n",
    "        self.net.set_grad(False)\n",
    "        self.l2_loss = nn.MSELoss(reduction='sum')\n",
    "        self.tv_loss = Total_Variation_Loss()\n",
    "        self.alpha = alpha\n",
    "        self.beta = beta\n",
    "        self.gamma = gamma\n",
    "        self.lambda_f = lambda_f\n",
    "        self.lambda_o = lambda_o\n",
    "\n",
    "    def construct(self, sample, style_gram_matrices):\n",
    "        \"\"\"Calculate ReCoNet loss.\"\"\"\n",
    "        occlusion_mask = occlusion_mask_from_flow(\n",
    "            sample[\"optical_flow\"],\n",
    "            sample[\"reverse_optical_flow\"],\n",
    "            sample[\"motion_boundaries\"])\n",
    "\n",
    "        # ReCoNet encode and decode\n",
    "        reconet_input = sample[\"frame\"] * 2 - 1\n",
    "        feature_maps = self.backbone.encoder(reconet_input)\n",
    "        output_frame = self.backbone.decoder(feature_maps)\n",
    "\n",
    "        previous_input = sample[\"pre_frame\"] * 2 - 1\n",
    "        previous_feature_maps = self.backbone.encoder(previous_input)\n",
    "        previous_output_frame = self.backbone.decoder(previous_feature_maps)\n",
    "\n",
    "        # Compute VGG features\n",
    "        vgg_input_frame = preprocess_for_vgg(sample[\"frame\"])\n",
    "        vgg_output_frame = preprocess_for_vgg((output_frame + 1) / 2)\n",
    "        input_vgg_features = self.net.encode(vgg_input_frame)\n",
    "        output_vgg_features = self.net.encode(vgg_output_frame)\n",
    "\n",
    "        vgg_previous_input_frame = preprocess_for_vgg(sample[\"pre_frame\"])\n",
    "        vgg_previous_output_frame = preprocess_for_vgg((previous_output_frame + 1) / 2)\n",
    "        previous_input_vgg_features = self.net.encode(vgg_previous_input_frame)\n",
    "        previous_output_vgg_features = self.net.encode(vgg_previous_output_frame)\n",
    "\n",
    "        # compute loss\n",
    "        content_loss = self.content_loss(output_vgg_features[2], input_vgg_features[2]) + \\\n",
    "                       self.content_loss(previous_output_vgg_features[2], previous_input_vgg_features[2])\n",
    "        style_loss = self.style_loss(output_vgg_features, style_gram_matrices) + \\\n",
    "                     self.style_loss(previous_output_vgg_features, style_gram_matrices)\n",
    "        total_var_loss = self.tv_loss(output_frame) + self.tv_loss(previous_output_frame)\n",
    "        f_temp_loss = self.feature_temporal_loss(feature_maps, previous_feature_maps,\n",
    "                                                 sample[\"reverse_optical_flow\"],\n",
    "                                                 occlusion_mask)\n",
    "        o_temp_loss = self.output_temporal_loss(reconet_input, previous_input,\n",
    "                                                output_frame, previous_output_frame,\n",
    "                                                sample[\"reverse_optical_flow\"],\n",
    "                                                occlusion_mask)\n",
    "\n",
    "        return self.alpha * content_loss + \\\n",
    "               self.beta * style_loss + \\\n",
    "               self.gamma * total_var_loss + \\\n",
    "               self.lambda_f * f_temp_loss + \\\n",
    "               self.lambda_o * o_temp_loss\n",
    "\n",
    "    def content_loss(self, output_feature, input_feature):\n",
    "        \"\"\"Content loss\"\"\"\n",
    "        _, c, h, w = output_feature.shape\n",
    "        return self.l2_loss(output_feature, input_feature) / (c * h * w)\n",
    "\n",
    "    def style_loss(self, vgg_feature_gram, style_gram):\n",
    "        \"\"\"Style loss\"\"\"\n",
    "        loss = 0\n",
    "        for content_fm, style_gm in zip(vgg_feature_gram, style_gram):\n",
    "            loss += self.l2_loss(gram_matrix(content_fm), style_gm)\n",
    "        return loss\n",
    "\n",
    "    def feature_temporal_loss(self, feature_maps, previous_feature_maps, reverse_optical_flow, occlusion_mask):\n",
    "        \"\"\"Feature temporal loss\"\"\"\n",
    "        _, c, h, w = feature_maps.shape\n",
    "\n",
    "        reverse_optical_flow_resized = resize_optical_flow(reverse_optical_flow, h, w)\n",
    "        occlusion_mask_resized = ops.ResizeNearestNeighbor((h, w))(occlusion_mask)\n",
    "        feature_maps = occlusion_mask_resized * feature_maps\n",
    "        pre_feature_maps = occlusion_mask_resized * warp_optical_flow(previous_feature_maps, reverse_optical_flow_resized)\n",
    "        loss = self.l2_loss(feature_maps, pre_feature_maps) / (c * h * w)\n",
    "        return loss\n",
    "\n",
    "    def output_temporal_loss(self, input_frame, previous_input_frame, output_frame, previous_output_frame,\n",
    "                             reverse_optical_flow, occlusion_mask):\n",
    "        \"\"\"Output temporal loss\"\"\"\n",
    "        input_diff = input_frame - warp_optical_flow(previous_input_frame, reverse_optical_flow)\n",
    "        output_diff = output_frame - warp_optical_flow(previous_output_frame, reverse_optical_flow)\n",
    "        luminance_input_diff = rgb_to_luminance(input_diff)\n",
    "        luminance_input_diff = ops.ExpandDims()(luminance_input_diff, 1)\n",
    "        _, _, h, w = input_frame.shape\n",
    "        loss = self.l2_loss(occlusion_mask * output_diff, occlusion_mask * luminance_input_diff) / (h * w)\n",
    "        return loss\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## 模型训练\n",
    "\n",
    "在前几个章节，已经介绍了数据集准备和模型准备，接下来就可以进行训练，在训练前需要先在reconet目录下创建model文件夹，作为模型的输出。接下来需要载入数据集，初始化模型，载入VGG16预训练模型，并实例化优化器，ReCoNet使用Adam优化器。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mindspore\n",
    "import mindspore.nn as nn\n",
    "\n",
    "from mindspore import context\n",
    "from src.model.loss import ReCoNetWithLoss\n",
    "from src.model.reconet import ReCoNet\n",
    "from src.model.vgg import vgg16\n",
    "from src.dataset.dataset import load_dataset\n",
    "from src.utils.reconet_utils import vgg_encode_image\n",
    "context.set_context(mode=context.PYNATIVE_MODE)\n",
    "\n",
    "\n",
    "monkaa_path = './dataset/Monkaa'\n",
    "ft3d_path = './dataset/Flyingthings3d'\n",
    "\n",
    "train_dataset = load_dataset(monkaa_path, ft3d_path)\n",
    "step_size = train_dataset.get_dataset_size()\n",
    "print('dataset size is {}'.format(step_size))\n",
    "\n",
    "vgg = './model/vgg16.ckpt'\n",
    "\n",
    "# Create model.\n",
    "reconet = ReCoNet()\n",
    "vgg_net = vgg16(vgg)\n",
    "\n",
    "style_file = './test_images/styles/mosaic.jpg'\n",
    "style_gram_matrices = vgg_encode_image(vgg_net, style_file)\n",
    "\n",
    "alpha = 1e4\n",
    "beta = 1e5\n",
    "gamma = 1e-5\n",
    "lambda_f = 1e5\n",
    "lambda_o = 2e5\n",
    "learning_rate = 0.001\n",
    "\n",
    "model = ReCoNetWithLoss(reconet,\n",
    "                        vgg_net,\n",
    "                        alpha,\n",
    "                        beta,\n",
    "                        gamma,\n",
    "                        lambda_f,\n",
    "                        lambda_o)\n",
    "\n",
    "# adam optimizer\n",
    "optim = nn.Adam(reconet.trainable_params(), learning_rate=learning_rate, weight_decay=0.0)\n",
    "\n",
    "train_net = nn.TrainOneStepCell(model, optim)\n",
    "\n",
    "global_step = 0\n",
    "epochs = 2\n",
    "\n",
    "# train by steps\n",
    "for epoch in range(epochs):\n",
    "    for sample in train_dataset.create_dict_iterator():\n",
    "        loss = train_net(sample, style_gram_matrices)\n",
    "\n",
    "        last_iteration = global_step == step_size // 2 * epochs - 1\n",
    "        if global_step % 25 == 0 or last_iteration:\n",
    "            print(f\"Epoch: [{epoch} / {epochs}], \"\n",
    "                  f\"step: [{global_step} / {step_size * epochs - 1}], \"\n",
    "                  f\"loss: {loss}\")\n",
    "        global_step += 1\n",
    "\n",
    "reconet_model = './model/reconet.ckpt'\n",
    "# save trained model\n",
    "mindspore.save_checkpoint(reconet, reconet_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### 模型推理\n",
    "\n",
    "当训练结束后，就可以使用任意一张图片，来测试风格迁移的效果\n",
    "\n",
    "ReCoNet支持图片和视频风格迁移，仅需要修改代码中的mode为'video'，并将output_file修改为output.mp4。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "from mindspore.train import Model\n",
    "\n",
    "from src.model.reconet import load_reconet\n",
    "from src.utils.reconet_utils import preprocess, save_infer_result, postprocess, batch_style_transfer\n",
    "\n",
    "\n",
    "def infer_image(input_file, output_file, model):\n",
    "    \"\"\"\n",
    "    Infer for image\n",
    "\n",
    "    Args:\n",
    "        input_file (str): input file name\n",
    "        output_file (str): output file name\n",
    "        model (ReCoNet): ReCoNet model\n",
    "    \"\"\"\n",
    "\n",
    "    # preprocess input image\n",
    "    image = preprocess(input_file)\n",
    "\n",
    "    # style input image\n",
    "    styled_image = model.predict(image).squeeze()\n",
    "\n",
    "    # post process and save image to the output file\n",
    "    save_infer_result((styled_image + 1) / 2, output_file)\n",
    "\n",
    "\n",
    "def init_video_cap(input, output):\n",
    "    \"\"\"\n",
    "    Infer for image\n",
    "\n",
    "    Args:\n",
    "        input (str): input file name\n",
    "        output (str): output file name\n",
    "\n",
    "    Returns:\n",
    "        capture, video capture\n",
    "        write, video writer\n",
    "        ret, whether have next frame\n",
    "        img, image frame\n",
    "    \"\"\"\n",
    "    capture = cv2.VideoCapture(input)\n",
    "    ret, img = capture.read()\n",
    "    height, width = img.shape[:2]\n",
    "    writer = cv2.VideoWriter(output, cv2.VideoWriter_fourcc(*'mp4v'), int(capture.get(cv2.CAP_PROP_FPS)),\n",
    "                             (width, height))\n",
    "    capture.set(cv2.CAP_PROP_POS_FRAMES, 0)\n",
    "    return capture, writer, ret, img\n",
    "\n",
    "\n",
    "def infer_video(input_file, output_file, model):\n",
    "    \"\"\"\n",
    "    Infer for video\n",
    "\n",
    "    Args:\n",
    "        input_file (str): input file name\n",
    "        output_file (str): output file name\n",
    "        model (ReCoNet): ReCoNet model\n",
    "    \"\"\"\n",
    "    # init video capture and video writer\n",
    "    cap, writer, ret, img = init_video_cap(input_file, output_file)\n",
    "\n",
    "    batch = [img]\n",
    "\n",
    "    # transfer frame one by one\n",
    "    while ret:\n",
    "        ret, frame = cap.read()\n",
    "        if frame is None:\n",
    "            print('Empty frame.')\n",
    "            continue\n",
    "        batch.append(frame)\n",
    "\n",
    "        if batch.__len__() == 2:\n",
    "            input_batch = [cv2.cvtColor(frame, cv2.COLOR_BGR2RGB) for frame in batch]\n",
    "            for output_frame in batch_style_transfer(input_batch, model):\n",
    "                writer.write(cv2.cvtColor(postprocess(output_frame), cv2.COLOR_RGB2BGR))\n",
    "            batch = []\n",
    "\n",
    "    if batch.__len__() != 0:\n",
    "        input_batch = [cv2.cvtColor(frame, cv2.COLOR_BGR2RGB) for frame in batch]\n",
    "        for output_frame in batch_style_transfer(input_batch, model):\n",
    "            writer.write(cv2.cvtColor(postprocess(output_frame), cv2.COLOR_RGB2BGR))\n",
    "\n",
    "    # close video capture\n",
    "    cap.release()\n",
    "\n",
    "mode = 'image'\n",
    "input_file = './test_images/Lenna.jpg'\n",
    "model_file = './model/reconet.ckpt'\n",
    "output_file = './output.png'\n",
    "# Create model.\n",
    "network = load_reconet(ckpt_file=model_file)\n",
    "\n",
    "network.set_train(False)\n",
    "\n",
    "# Init the model.\n",
    "model = Model(network)\n",
    "\n",
    "print('Infer start in [{}] mode'.format(mode))\n",
    "\n",
    "if mode.lower() == 'image':\n",
    "    infer_image(input_file, output_file, model)\n",
    "else:\n",
    "    infer_video(input_file, output_file, model)\n",
    "\n",
    "print('infer done')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "推理结果如下:  \n",
    "![infer_result](./images/lenna_mosaic.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 更多风格模型\n",
    "\n",
    "我们在模型仓库中提供了基于8种风格的预训练模型，可从 <https://download.mindspore.cn/vision/reconet/>中下载，具体风格文件见test_images/styles。每个风格文件都在同名的文件下有已经训练好的风格文件，可以直接进行推理。如：风格candy对应的模型下载地址为<https://download.mindspore.cn/vision/reconet/candy/reconet_candy.ckpt>\n",
    "\n",
    "各模型推理结果与原风格图片对比如下，上方为推理结果，下方对应的风格图片：\n",
    "![infer_result](./images/infer_result.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## 算法流程\n",
    "\n",
    "![work_flow](images/work_flow.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## 总结\n",
    "\n",
    "ReCoNet是一个轻量化的高实时性风格迁移网络，在保证内容和风格相似的同时，还额外考虑了相邻帧的时序特征，因此在迁移视频时可以让视频迁移更连贯，具有更好的效果。此外，ReCoNet网络由于结构简单，层数较少，因此具有很快的训练和推理速度。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## 引用\n",
    "\n",
    "[1] C. Gao, D. Gu, F. Zhang, and Y. Yu, “ReCoNet: Real-time Coherent Video Style Transfer Network,” arXiv:1807.01197 [cs], Nov. 2018, Accessed: Apr. 15, 2022. [Online]. Available: http://arxiv.org/abs/1807.01197\n",
    "\n",
    "[2] J. Johnson, A. Alahi, and L. Fei-Fei, “Perceptual Losses for Real-Time Style Transfer and Super-Resolution.” arXiv, Mar. 26, 2016. Accessed: Jun. 29, 2022. [Online]. Available: http://arxiv.org/abs/1603.08155\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mindspore",
   "language": "python",
   "name": "mindspore"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
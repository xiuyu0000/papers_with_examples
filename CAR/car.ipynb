{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# 基于内容自适应重采样(CAR)的学习图像降采样方法\n",
    "\n",
    "CAR算法是一种高效的图像下采样和上采样方法，能够帮助图像数据的存储并减小图像传输所需带宽，同时不损失图像的细节。算法设计了一个重采样网络，用于生成低分辨率图像，同时引入了一个可差分的超分辨率网络来恢复低分辨率图像，通过重构损失来更新整个模型的参数。实验证明，该算法达到了最先进的超分辨率性能。\n",
    "\n",
    "# 模型简介\n",
    "\n",
    "![show_images](images/model.jpg)\n",
    "\n",
    "如上图所示，CAR算法采用ResamplerNet生成下采样图像所需的权重与偏移，ResamplerNet由卷积和残差块组成。得到权重与偏移后，通过Downscaling进行图像下采样，下采样过程由cuda实现，然后将下采样图像通过超分辨率网络恢复，最后将恢复后的图像和原始图像通过L1范数对比，得到重构损失，并更新网络参数。\n",
    "\n",
    "## 数据处理\n",
    "\n",
    "开始实验之前，请确保本地已经安装了Python环境并安装了MindSpore Vision套件。\n",
    "\n",
    "## 数据准备\n",
    "\n",
    "训练数据采用DIV2K中的高清图像，训练集包含800张高清图像，验证集包含100张高清图像。\n",
    "训练集下载地址：http://data.vision.ee.ethz.ch/cvl/DIV2K/DIV2K_train_HR.zip\n",
    "验证集下载地址：http://data.vision.ee.ethz.ch/cvl/DIV2K/DIV2K_valid_HR.zip\n",
    "请将解压后的数据集放到./datasets/DIV2K/下，文件目录如下所示：\n",
    "\n",
    "```text\n",
    "\n",
    ".datasets/\n",
    "    └── DIV2K\n",
    "            ├── DIV2K_train_HR\n",
    "            |    ├── 0001.png\n",
    "            |    ├── 0002.png\n",
    "            |    ├── ...\n",
    "            ├── DIV2K_valid_HR\n",
    "            |    ├── 000801.png\n",
    "            |    ├── 000802.png\n",
    "            |    ├── ...\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import argparse\n",
    "from typing import Optional, Callable\n",
    "\n",
    "import numpy as np\n",
    "import mindspore as ms\n",
    "import mindspore.dataset as ds\n",
    "from mindspore import context, nn, ops, Model\n",
    "from mindspore.dataset import vision\n",
    "from mindvision.io.images import imread\n",
    "from mindvision.check_param import Validator\n",
    "\n",
    "class DIV2KHR:\n",
    "    def __init__(self, path: str, split: str):\n",
    "        Validator.check_string(split, [\"train\", \"valid\"], \"split\")\n",
    "        realpath = os.path.realpath(path)\n",
    "        self.path = os.path.join(realpath, f\"DIV2K_{split}_HR\")\n",
    "        data = os.listdir(self.path)\n",
    "        self.data_list = [os.path.join(self.path, idx) for idx in data]\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        \"\"\" Get a list of datasets \"\"\"\n",
    "        return imread(self.data_list[index], 'RGB')\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\" Get the length of each line \"\"\"\n",
    "        return len(self.data_list)\n",
    "\n",
    "\n",
    "def default_transform(image):\n",
    "    image = np.asarray(image)\n",
    "    height, width, _ = image.shape\n",
    "    image = image[:height // 8 * 8, :width // 8 * 8, :] # 输入图像必须保证长宽8对齐\n",
    "    image = vision.py_transforms.ToTensor()(image)\n",
    "\n",
    "    return image\n",
    "\n",
    "def build_dataset(dataset,\n",
    "                  batch_size: int = 1,\n",
    "                  repeat_num: int = 1,\n",
    "                  shuffle: Optional[bool] = False,\n",
    "                  num_parallel_workers: Optional[int] = 1,\n",
    "                  num_shards: Optional[int] = None,\n",
    "                  shard_id: Optional[int] = None,\n",
    "                  transform: Optional[Callable] = default_transform):\n",
    "    dataset = ds.GeneratorDataset(dataset, ['image'],\n",
    "                                  num_parallel_workers=num_parallel_workers,\n",
    "                                  shuffle=shuffle,\n",
    "                                  num_shards=num_shards,\n",
    "                                  shard_id=shard_id)\n",
    "    if transform:\n",
    "        dataset = dataset.map(operations=transform,\n",
    "                              input_columns='image',\n",
    "                              num_parallel_workers=num_parallel_workers)\n",
    "\n",
    "    dataset = dataset.batch(batch_size, drop_remainder=True)\n",
    "    dataset = dataset.repeat(repeat_num)\n",
    "\n",
    "    return dataset\n",
    "\n",
    "# 初始化参数\n",
    "parser = argparse.ArgumentParser(description='Train CAR')\n",
    "parser.add_argument('--image_path', default='./datasets/DIV2K', type=str) #数据集路径\n",
    "parser.add_argument('-j', '--workers', default=1, type=int)\n",
    "parser.add_argument('--device_target', default='GPU', choices=['CPU', 'GPU', 'Ascend'], type=str)\n",
    "parser.add_argument('--end_epoch', default=500, type=int)\n",
    "parser.add_argument('--train_batchsize', default=8, type=int)\n",
    "parser.add_argument('--train_repeat_num', default=1, type=int)\n",
    "parser.add_argument('--train_resize', default=192, type=int)  # 4倍下采样\n",
    "parser.add_argument('--scale', default=4, type=int, help='downscale factor')\n",
    "parser.add_argument('--eval_proid', default=1, type=int)\n",
    "parser.add_argument('--checkpoint_path', default='./checkpoint', type=str)\n",
    "parser.add_argument('--output_dir', type=str, default='./exp_res', help='path to store results')\n",
    "args = parser.parse_known_args()[0]\n",
    "\n",
    "context.set_context(mode=context.GRAPH_MODE, device_target=args.device_target, device_id=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 数据增强\n",
    "\n",
    "训练过程中，将每张高清图像随机裁剪到192×192(4倍下采样)或96×96(2倍下采样)，训练过程只采用随机水平翻转和随机垂直翻转。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 创建训练数据集\n",
    "train_transform = [vision.c_transforms.RandomCrop(args.train_resize),\n",
    "                   vision.c_transforms.RandomHorizontalFlip(),\n",
    "                   vision.c_transforms.RandomVerticalFlip(),\n",
    "                   vision.py_transforms.ToTensor()]\n",
    "\n",
    "train_dataloader = build_dataset(DIV2KHR(args.image_path, \"train\"),\n",
    "                                 batch_size=args.train_batchsize,\n",
    "                                 repeat_num=args.train_repeat_num,\n",
    "                                 shuffle=True,\n",
    "                                 transform=train_transform)\n",
    "step_size = train_dataloader.get_dataset_size()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 构建网络\n",
    "\n",
    "ResamplerNet网络中使用3x3卷积和LeakyReLU将特征升维到128，然后使用5个残差结构提取特征，最后用两个相同的结构分支计算采样权重和偏移，分支由‘Conv-LeakyReLU’ 对组成，并且将特征维度升至256。超分辨率网络采用EDSR，由32个残差结构组成，每个残差结构的特征维度为256."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mindspore.nn as nn\n",
    "import mindspore.ops as ops\n",
    "\n",
    "from src.model.block import ResBlock, default_conv, Upsampler\n",
    "from src.model.block import MeanShift, NormalizeBySum, DownsampleBlock, ReflectionPad2d, UpsampleBlock, ResidualBlock\n",
    "\n",
    "class DSN(nn.Cell):\n",
    "    def __init__(self, k_size, input_channels=3, scale=4):\n",
    "        super().__init__()\n",
    "\n",
    "        self.k_size = k_size\n",
    "        self.sub_mean = MeanShift(1)\n",
    "        self.normalize = NormalizeBySum()\n",
    "\n",
    "        self.ds_1 = nn.SequentialCell(\n",
    "            ReflectionPad2d(2),\n",
    "            nn.Conv2d(input_channels, 64, 5, pad_mode=\"valid\", has_bias=True),\n",
    "            nn.LeakyReLU(0.2),\n",
    "        )\n",
    "\n",
    "        self.ds_2 = DownsampleBlock(2, 64, 128, ksize=1)\n",
    "        self.ds_4 = DownsampleBlock(2, 128, 128, ksize=1)\n",
    "\n",
    "        res_4 = []\n",
    "        for _ in range(5):\n",
    "            res_4 += [ResidualBlock(128, 128)]\n",
    "        self.res_4 = nn.SequentialCell(*res_4)\n",
    "\n",
    "        self.ds_8 = DownsampleBlock(2, 128, 256)\n",
    "\n",
    "        self.kernels_trunk = nn.SequentialCell(\n",
    "            ReflectionPad2d(1),\n",
    "            nn.Conv2d(256, 256, 3, pad_mode=\"pad\", has_bias=True),\n",
    "            nn.ReLU(),\n",
    "            ReflectionPad2d(1),\n",
    "            nn.Conv2d(256, 256, 3, pad_mode=\"pad\", has_bias=True),\n",
    "            nn.ReLU(),\n",
    "            ReflectionPad2d(1),\n",
    "            nn.Conv2d(256, 256, 3, pad_mode=\"pad\", has_bias=True),\n",
    "            nn.ReLU(),\n",
    "            UpsampleBlock(8 // scale, 256, 256, ksize=1),\n",
    "            ReflectionPad2d(1),\n",
    "            nn.Conv2d(256, 256, 3, pad_mode=\"pad\", has_bias=True),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "\n",
    "        self.kernels_weight = nn.SequentialCell(\n",
    "            ReflectionPad2d(1),\n",
    "            nn.Conv2d(256, 256, 3, pad_mode=\"pad\", has_bias=True),\n",
    "            nn.ReLU(),\n",
    "            ReflectionPad2d(1),\n",
    "            nn.Conv2d(256, k_size**2, 3, pad_mode=\"pad\", has_bias=True),\n",
    "        )\n",
    "\n",
    "        self.offsets_trunk = nn.SequentialCell(\n",
    "            ReflectionPad2d(1),\n",
    "            nn.Conv2d(256, 256, 3, pad_mode=\"valid\", has_bias=True),\n",
    "            nn.ReLU(),\n",
    "            ReflectionPad2d(1),\n",
    "            nn.Conv2d(256, 256, 3, pad_mode=\"valid\", has_bias=True),\n",
    "            nn.ReLU(),\n",
    "            ReflectionPad2d(1),\n",
    "            nn.Conv2d(256, 256, 3, pad_mode=\"valid\", has_bias=True),\n",
    "            nn.ReLU(),\n",
    "            UpsampleBlock(8 // scale, 256, 256, ksize=1),\n",
    "            ReflectionPad2d(1),\n",
    "            nn.Conv2d(256, 256, 3, pad_mode=\"valid\", has_bias=True),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "\n",
    "        self.offsets_h_generation = nn.SequentialCell(\n",
    "            ReflectionPad2d(1),\n",
    "            nn.Conv2d(256, 256, 3, pad_mode=\"valid\", has_bias=True),\n",
    "            nn.ReLU(),\n",
    "            ReflectionPad2d(1),\n",
    "            nn.Conv2d(256, k_size**2, 3, pad_mode=\"valid\", has_bias=True),\n",
    "            nn.Tanh(),\n",
    "        )\n",
    "\n",
    "        self.offsets_v_generation = nn.SequentialCell(\n",
    "            ReflectionPad2d(1),\n",
    "            nn.Conv2d(256, 256, 3, pad_mode=\"valid\", has_bias=True),\n",
    "            nn.ReLU(),\n",
    "            ReflectionPad2d(1),\n",
    "            nn.Conv2d(256, k_size**2, 3, pad_mode=\"valid\", has_bias=True),\n",
    "            nn.Tanh(),\n",
    "        )\n",
    "\n",
    "    def construct(self, img):\n",
    "        x = self.sub_mean(img)\n",
    "\n",
    "        x = self.ds_1(x)\n",
    "        x = self.ds_2(x)\n",
    "        x = self.ds_4(x)\n",
    "        x = x + self.res_4(x)\n",
    "        x = self.ds_8(x)\n",
    "\n",
    "        kt = self.kernels_trunk(x)\n",
    "\n",
    "        kt = self.kernels_weight(kt)\n",
    "        k_weight = ops.clip_by_value(kt, 1e-6, 1)\n",
    "        kernels = self.normalize(k_weight)\n",
    "\n",
    "        ot = self.offsets_trunk(x)\n",
    "        offsets_h = self.offsets_h_generation(ot)\n",
    "        offsets_v = self.offsets_v_generation(ot)\n",
    "\n",
    "        return kernels, offsets_h, offsets_v\n",
    "\n",
    "class EDSR(nn.Cell):\n",
    "    \"\"\"\n",
    "    Upscaling module to guide the training of the proposed CAR model.\n",
    "\n",
    "    Args:\n",
    "        n_resblocks(int): The number of net blocks. Default: 16.\n",
    "        n_feats(int): The hidden layer features dimensions. default: 64.\n",
    "        scale(int): Upscaling rate. Default: 4.\n",
    "        conv(Cell): The convolution layer. Default: default_conv.\n",
    "\n",
    "    Inputs:\n",
    "        - **x** (Tensor) - The downscale image tensors.\n",
    "\n",
    "    Outputs:\n",
    "        Tensor, The super-resolution image tensors.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, n_resblocks=16, n_feats=64, scale=4, conv=default_conv):\n",
    "        super(EDSR, self).__init__()\n",
    "\n",
    "        kernel_size = 3\n",
    "        act = nn.ReLU()\n",
    "        self.sub_mean = MeanShift(1)\n",
    "        self.add_mean = MeanShift(1, sign=1)\n",
    "\n",
    "        # define head module\n",
    "        m_head = [conv(3, n_feats, kernel_size)]\n",
    "\n",
    "        # define body module\n",
    "        m_body = [\n",
    "            ResBlock(conv, n_feats, kernel_size, act=act, res_scale=0.1)\n",
    "            for _ in range(n_resblocks)\n",
    "        ]\n",
    "        m_body.append(conv(n_feats, n_feats, kernel_size))\n",
    "\n",
    "        # define tail module\n",
    "        m_tail = [\n",
    "            Upsampler(conv, scale, n_feats, act=False),\n",
    "            conv(n_feats, 3, kernel_size),\n",
    "        ]\n",
    "        self.head = nn.SequentialCell(m_head)\n",
    "        self.body = nn.SequentialCell(*m_body)\n",
    "        self.tail = nn.SequentialCell(*m_tail)\n",
    "\n",
    "    def construct(self, x):\n",
    "        x = self.sub_mean(x)\n",
    "        x = self.head(x)\n",
    "        res = self.body(x)\n",
    "        res += x\n",
    "        x = self.tail(res)\n",
    "        x = self.add_mean(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "scale = args.scale\n",
    "kernel_size = 3 * scale + 1\n",
    "\n",
    "# create model\n",
    "kernel_generation_net = DSN(k_size=kernel_size, scale=scale)  # ResamplerNet\n",
    "upscale_net = EDSR(32, 256, scale=scale)    # SRNet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 下采样过程\n",
    "\n",
    "下采样过程通过cuda编程实现，mindspore采用aot方式添加自定义算子，需要执行以下命令\n",
    "\n",
    "```sh\n",
    "cd codebase/course/application_example/CAR/src/plug_in/adaptive_gridsampler\n",
    "python setup.py\n",
    "```\n",
    "\n",
    "编译完成后，可以生成so文件，即可正常导入下采样算子。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mindspore.ops as ops\n",
    "from mindspore.nn import Cell\n",
    "from mindspore.ops import DataType, CustomRegOp\n",
    "\n",
    "from src.model.block import ReflectionPad2d\n",
    "\n",
    "so_path = \"./src/plug_in/adaptive_gridsampler/adaptive_gridsampler_cuda.so\"\n",
    "\n",
    "sampler_gpu_info = (CustomRegOp()\n",
    "                    .input(0, \"img\")\n",
    "                    .input(1, \"kernels\")\n",
    "                    .input(2, \"offsets_h\")\n",
    "                    .input(3, \"offsets_v\")\n",
    "                    .input(4, \"offset_unit\")\n",
    "                    .input(5, \"padding\")\n",
    "                    .output(0, \"output\")\n",
    "                    .dtype_format(DataType.F32_Default,\n",
    "                                  DataType.F32_Default,\n",
    "                                  DataType.F32_Default,\n",
    "                                  DataType.F32_Default,\n",
    "                                  DataType.None_Default,\n",
    "                                  DataType.None_Default,\n",
    "                                  DataType.F32_Default)\n",
    "                    .target(\"GPU\")\n",
    "                    .get_op_info())\n",
    "\n",
    "sampler_bprop_gpu_info = (CustomRegOp()\n",
    "                          .input(0, \"img\")\n",
    "                          .input(1, \"kernels\")\n",
    "                          .input(2, \"offsets_h\")\n",
    "                          .input(3, \"offsets_v\")\n",
    "                          .input(4, \"offset_unit\")\n",
    "                          .input(5, \"padding\")\n",
    "                          .input(6, \"grad_output\")\n",
    "                          .output(0, \"grad_k\")\n",
    "                          .output(1, \"grad_oh\")\n",
    "                          .output(2, \"grad_ob\")\n",
    "                          .dtype_format(DataType.F32_Default,\n",
    "                                        DataType.F32_Default,\n",
    "                                        DataType.F32_Default,\n",
    "                                        DataType.F32_Default,\n",
    "                                        DataType.None_Default,\n",
    "                                        DataType.None_Default,\n",
    "                                        DataType.F32_Default,\n",
    "                                        DataType.F32_Default,\n",
    "                                        DataType.F32_Default,\n",
    "                                        DataType.F32_Default)\n",
    "                          .target(\"GPU\")\n",
    "                          .get_op_info())\n",
    "\n",
    "\n",
    "def infer_shape_backward(x1, x2, x3, x4, x5, x6, x7):\n",
    "    _ = (x1, x3, x4, x5, x6, x7)\n",
    "    return (x2, x2, x2)\n",
    "\n",
    "\n",
    "def infer_type_backward(x1, x2, x3, x4, x5, x6, x7):\n",
    "    _ = (x2, x3, x4, x5, x6, x7)\n",
    "    return (x1, x1, x1)\n",
    "\n",
    "\n",
    "aot_bprop = ops.Custom(\n",
    "    so_path + \":adaptive_gridsampler_cuda_backward\",\n",
    "    infer_shape_backward,\n",
    "    infer_type_backward,\n",
    "    \"aot\",\n",
    "    reg_info=sampler_bprop_gpu_info,\n",
    ")\n",
    "\n",
    "\n",
    "def backward(img, kernels, offsets_h, offsets_v, offset_unit, padding, out, dout):\n",
    "    _ = out\n",
    "    input_img = img[..., padding:-padding, padding:-padding]\n",
    "    grad_k, grad_h, grad_v = aot_bprop(\n",
    "        input_img, kernels, offsets_h, offsets_v, offset_unit, padding, dout\n",
    "    )\n",
    "\n",
    "    return (ops.ZerosLike()(img), grad_k, grad_h, grad_v, None, None)\n",
    "\n",
    "\n",
    "def infer_shape_forward(x1, x2, x3, x4, x5, x6):\n",
    "    _ = (x3, x4, x5, x6)\n",
    "    shape = (x1[0], x1[1], x2[2], x2[3])\n",
    "    return shape\n",
    "\n",
    "\n",
    "def infer_type_forward(x1, x2, x3, x4, x5, x6):\n",
    "    _ = (x2, x3, x4, x5, x6)\n",
    "    return x1\n",
    "\n",
    "\n",
    "class Downsampler(Cell):\n",
    "    \"\"\"\n",
    "    Downsampler\n",
    "    \"\"\"\n",
    "    def __init__(self, k_size):\n",
    "        super().__init__()\n",
    "        self.k_size = k_size\n",
    "        self.ops = ops.Custom(\n",
    "            so_path + \":adaptive_gridsampler_cuda_forward\",\n",
    "            infer_shape_forward,\n",
    "            infer_type_forward,\n",
    "            \"aot\",\n",
    "            backward,\n",
    "            sampler_gpu_info,\n",
    "        )\n",
    "\n",
    "    def construct(self, img, kernels, offsets_h, offsets_v, offset_unit):\n",
    "        padding = self.k_size // 2\n",
    "        img = ReflectionPad2d(padding)(img)\n",
    "\n",
    "        return self.ops(img, kernels, offsets_h, offsets_v, offset_unit, padding)\n",
    "\n",
    "downsampler_net = Downsampler(kernel_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 量化过程\n",
    "\n",
    "下采样后的数据为连续的浮点类型，而一般图像使用0-255的整型数据表示像素值，将浮点类型量化为整型数据的过程是一个不可导的过程，为了能对整个网络端到端的求导，论文中采用soft round函数拟合下采样过程。公式如下：\n",
    "\n",
    "$$\n",
    "round_{soft}(x)=x-α*\\frac{\\sin(2\\pi x)}{2\\pi}\n",
    "$$\n",
    "\n",
    "该函数仅在反向传播时用于求导，计算其导函数可得：\n",
    "\n",
    "$$\n",
    "round_{soft}'(x)=1-α*\\cos(2\\pi x)\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Quantization(nn.Cell):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.round = ops.Round()\n",
    "        self.clip = ops.clip_by_value\n",
    "        self.cos = ops.Cos()\n",
    "        self.pi = 3.1415926\n",
    "        self.alp = 1.\n",
    "\n",
    "    def construct(self, img):\n",
    "        img = self.clip(img, 0, 1)\n",
    "        img = img * 255\n",
    "        img = self.round(img)\n",
    "        return img / 255\n",
    "\n",
    "    def bprop(self, img, out, grad_output):\n",
    "        _ = out\n",
    "\n",
    "        grad_input = grad_output\n",
    "        grad_input = grad_output*(1-self.alp*self.cos(2*self.pi*img))\n",
    "        return (grad_input,)\n",
    "quant = Quantization()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 损失函数\n",
    "\n",
    "论文中采用了三种损失函数，分别是L1loss、offsetloss 和 partial TV loss\n",
    "\n",
    "### L1loss\n",
    "\n",
    "L1loss定义为恢复后的图像和原始图像的L1范数。\n",
    "\n",
    "$$\n",
    "\\frac{1}{N} \\sum_{\\boldsymbol{p} \\in \\mathbf{I}}|\\boldsymbol{p}-\\hat{\\boldsymbol{p}}|\n",
    "$$\n",
    "\n",
    "其中 $\\hat{\\mathbf{I}}$ 表示超分辨率结果, $\\boldsymbol{p}$ 和 $\\hat{\\boldsymbol{p}}$ 分别表示ground-truth和重构像素值, N 为像素点数量和颜色通道的乘积。\n",
    "\n",
    "### offsetloss\n",
    "\n",
    "offsetloss用于保证下采样后的图片仍然有很好的拓扑结构。对于下采样后的每个点，远离采样中心的像素和采样点的相关性更低，通过offsetloss约束偏移矩阵的权重。\n",
    "\n",
    "$$\\sum_{i=0}^{m-1} \\sum_{j=0}^{n-1} \\eta+\\sqrt{\\Delta X_{x, y}(i, j)^{2}+\\Delta Y_{x, y}(i, j)^{2}} \\cdot w(i, j)$$\n",
    "\n",
    "其中$w(i, j) = \\sqrt{\\left(i-\\frac{m}{2}\\right)^{2}+\\left(j-\\frac{n}{2}\\right)^{2}} / \\sqrt{\\frac{m}{2}^{2}+\\frac{n^{2}}{2}}$，(m, n)表示采样中心坐标，(i, j)表示偏移矩阵(i, j)位置的值，$w(i, j)$为(i, j)到(m, n)的距离。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class OffsetLoss(nn.Cell):\n",
    "    def __init__(self, kernel_size=13, offsetloss_weight=1.):\n",
    "        super(OffsetLoss, self).__init__()\n",
    "        self.offsetloss_weight = offsetloss_weight # loss 权重\n",
    "        x = ms.numpy.arange(0, kernel_size, dtype=ms.float32)\n",
    "        y = ms.numpy.arange(0, kernel_size, dtype=ms.float32)\n",
    "        x_m, y_m = ops.Meshgrid()((x, y))\n",
    "        self.sqrt = ops.Sqrt()\n",
    "        weight = self.sqrt((x_m-kernel_size/2)**2 + (y_m-kernel_size/2)**2)/kernel_size\n",
    "        self.weight = weight.view(1, kernel_size**2, 1, 1)\n",
    "\n",
    "    def construct(self, offsets_h, offsets_v):\n",
    "        b, _, h, w = offsets_h.shape\n",
    "        loss = self.sqrt(offsets_h * offsets_h + offsets_v * offsets_v)*self.weight\n",
    "        return self.offsetloss_weight*loss.sum()/(h * w * b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### partial TV loss\n",
    "\n",
    "相邻采样核偏移不一致可能导致下采样图像的像素相移，表现为锯齿状，特别是在垂直和水平的尖锐边缘。因此引入 partial TV loss保证偏移的一致性。\n",
    "\n",
    "$$\n",
    "Loss^{TV} = \\sum_{x, y}\\left(\\sum_{i, j}\\left|\\Delta X_{\\cdot, y+1}(i, j)-\\Delta X_{\\cdot, y}(i, j)\\right| \\cdot \\mathbf{K}(i, j) + \\sum_{i, j}\\left|\\Delta Y_{x+1, \\cdot}(i, j)-\\Delta Y_{x, \\cdot}(i, j)\\right| \\cdot \\mathbf{K}(i, j)\\right)\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TvLoss(nn.Cell):\n",
    "    def __init__(self, tvloss_weight=1):\n",
    "        super(TvLoss, self).__init__()\n",
    "        self.tvloss_weight = tvloss_weight  # loss 权重\n",
    "        self.abs = ops.Abs()\n",
    "\n",
    "    def construct(self, offsets_h, offsets_v, kernel):\n",
    "        batch, _, _, _ = offsets_h.shape\n",
    "        diff_1 = self.abs(offsets_v[..., 1:] - offsets_v[..., :-1]) * kernel[..., :-1]\n",
    "        diff_2 = self.abs(offsets_h[:, :, 1:, :] - offsets_h[:, :, :-1, :]) * kernel[:, :, :-1, :]\n",
    "        tv_loss = diff_1.sum()+diff_2.sum()\n",
    "        return self.tvloss_weight * tv_loss / batch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 创建训练网络\n",
    "\n",
    "将ResamplerNet，SRNet, 下采样过程, 量化过程和loss组合起来。构建训练网络，初始学习率$10^{−4}$ ,训练500epoch，每100个epoch降低学习率。优化器采用Adam， β1 = 0.9, β2 = 0.999\n",
    ", $\\epsilon$ = 10−6.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NetWithLoss(nn.Cell):\n",
    "    def __init__(self, net1, net2, aux_net1, aux_net2, offset, loss1, loss2):\n",
    "        super(NetWithLoss, self).__init__()\n",
    "        self.net1 = net1\n",
    "        self.net2 = net2\n",
    "        self.dsn = aux_net1\n",
    "        self.quant = aux_net2\n",
    "        self.offset_unit = offset\n",
    "        self.tv_loss = loss1\n",
    "        self.offset_loss = loss2\n",
    "        self.l1_loss = nn.L1Loss()\n",
    "\n",
    "    def construct(self, image):\n",
    "        kernels, offsets_h, offsets_v = self.net1(image)\n",
    "        downscaled_img = self.dsn(image, kernels, offsets_h, offsets_v, self.offset_unit)\n",
    "        downscaled_img = self.quant(downscaled_img)\n",
    "        reconstructed_img = self.net2(downscaled_img)\n",
    "        loss1 = self.l1_loss(reconstructed_img, image)\n",
    "        loss2 = self.tv_loss(offsets_h, offsets_v, kernels)\n",
    "        loss3 = self.offset_loss(offsets_h, offsets_v)\n",
    "\n",
    "        return loss1 + loss2 + loss3\n",
    "\n",
    "\n",
    "network = NetWithLoss(kernel_generation_net,\n",
    "                      upscale_net,\n",
    "                      downsampler_net,\n",
    "                      quant,\n",
    "                      scale,\n",
    "                      TvLoss(0.005),\n",
    "                      OffsetLoss(offsetloss_weight=0.001))\n",
    "\n",
    "num_epochs = args.end_epoch\n",
    "total_steps = step_size * num_epochs\n",
    "lr = nn.dynamic_lr.piecewise_constant_lr([int(0.2*total_steps), int(0.4*total_steps),\n",
    "                                          int(0.6*total_steps), int(0.8*total_steps),\n",
    "                                          total_steps],\n",
    "                                         [1e-4, 5e-5, 1e-5, 5e-6, 1e-6])\n",
    "opt_para = list(kernel_generation_net.trainable_params())+list(upscale_net.trainable_params())\n",
    "opt = nn.optim.Adam(opt_para, learning_rate=lr, eps=1e-6)\n",
    "model = Model(network=network, optimizer=opt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 创建评估网络\n",
    "\n",
    "评估网络采用DIV2KHR中验证集的10张图片，由于每张图片的大小不一致，因此测试集的batchsize设置为1。评估网络通过callback调用，并保存网络权重文件。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "from collections import OrderedDict\n",
    "\n",
    "from mindspore import ops, Tensor, save_checkpoint\n",
    "from mindspore.train.callback import Callback\n",
    "\n",
    "from src.utils.metric import cal_psnr\n",
    "\n",
    "class ValidateCell(nn.Cell):\n",
    "    def __init__(self, net1, net2, aux_net1, aux_net2, scale, offset):\n",
    "        super(ValidateCell, self).__init__()\n",
    "        self.net1 = net1\n",
    "        self.net2 = net2\n",
    "        self.dsn = aux_net1\n",
    "        self.quant = aux_net2\n",
    "        self.offset_unit = offset\n",
    "        self.scale = scale\n",
    "\n",
    "    def construct(self, image):\n",
    "        kernels, offsets_h, offsets_v = self.net1(image)\n",
    "        downscaled_img = self.dsn(image, kernels, offsets_h, offsets_v, self.offset_unit)\n",
    "        downscaled_img = self.quant(downscaled_img)\n",
    "        reconstructed_img = self.net2(downscaled_img)\n",
    "\n",
    "        return downscaled_img, reconstructed_img\n",
    "\n",
    "class SaveCheckpoint(Callback):\n",
    "    def __init__(self, eval_model, ds_eval, scale, save_path, eval_period=1):\n",
    "        \"\"\"init\"\"\"\n",
    "        super(SaveCheckpoint, self).__init__()\n",
    "        self.model = eval_model\n",
    "        self.ds_eval = ds_eval\n",
    "        self.m_psnr = 0.\n",
    "        self.eval_period = eval_period\n",
    "        path = os.path.realpath(save_path)\n",
    "        if not os.path.exists(path):\n",
    "            os.makedirs(path, exist_ok=True)\n",
    "        self.save_path = path\n",
    "        self.scale = scale\n",
    "\n",
    "    def epoch_end(self, run_context):\n",
    "        cb_params = run_context.original_args()\n",
    "        cur_epoch = cb_params.cur_epoch_num\n",
    "        scale = self.scale\n",
    "        psnr_list = []\n",
    "        if ((cur_epoch + 1) % self.eval_period) == 0:\n",
    "            print(\"Validating...\")\n",
    "            for i, data in enumerate(self.ds_eval.create_dict_iterator()):\n",
    "                if i > 10:\n",
    "                    break\n",
    "                image = data['image']\n",
    "                _, reconstructed_img = self.model(image)\n",
    "                image = image.asnumpy().transpose(0, 2, 3, 1)\n",
    "                orig_img = np.uint8(image * 255).squeeze()\n",
    "                reconstructed_img = ops.clip_by_value(reconstructed_img, 0, 1) * 255\n",
    "                reconstructed_img = reconstructed_img.asnumpy().transpose(0, 2, 3, 1)\n",
    "                recon_img = np.uint8(reconstructed_img).squeeze()\n",
    "\n",
    "                psnr = cal_psnr(orig_img[scale:-scale, scale:-scale, ...],\n",
    "                                recon_img[scale:-scale, scale:-scale, ...])\n",
    "                psnr_list.append(psnr)\n",
    "            m_psnr = np.mean(psnr_list)\n",
    "            if m_psnr > self.m_psnr:\n",
    "                self.m_psnr = m_psnr\n",
    "                save_path = os.path.join(self.save_path, f\"{self.scale}x\")\n",
    "                if not os.path.exists(save_path):\n",
    "                    os.mkdir(save_path)\n",
    "                net = cb_params.train_network\n",
    "                net.init_parameters_data()\n",
    "                param_dict = OrderedDict()\n",
    "                for _, param in net.parameters_and_names():\n",
    "                    param_dict[param.name] = param\n",
    "                param_kgn = []\n",
    "                param_usn = []\n",
    "                for (key, value) in param_dict.items():\n",
    "                    if \"net1\" in key:\n",
    "                        each_param = {\"name\": key.replace(\"net1.\", \"\")}\n",
    "                        param_data = Tensor(value.data.asnumpy())\n",
    "                        each_param[\"data\"] = param_data\n",
    "                        param_kgn.append(each_param)\n",
    "                    elif \"net2\" in key:\n",
    "                        each_param = {\"name\": key.replace(\"net2.\", \"\")}\n",
    "                        param_data = Tensor(value.data.asnumpy())\n",
    "                        each_param[\"data\"] = param_data\n",
    "                        param_usn.append(each_param)\n",
    "                save_checkpoint(param_kgn, os.path.join(save_path, \"kgn.ckpt\"))  # 将resampler和SRNet的权重分开保存\n",
    "                save_checkpoint(param_usn, os.path.join(save_path, \"usn.ckpt\"))\n",
    "                print(f\"epoce {cur_epoch}, Save model at {self.save_path}, m_psnr for 10 images: {m_psnr}\")\n",
    "            else:\n",
    "                print(f\"epoce {cur_epoch}, m_psnr for 10 images: {m_psnr}\")\n",
    "            print(\"Validating Done.\")\n",
    "\n",
    "    def end(self, run_context):\n",
    "        cb_params = run_context.original_args()\n",
    "        cur_epoch = cb_params.cur_epoch_num\n",
    "        print(f\"Finish training, totally epoches: {cur_epoch}, best psnr: {self.m_psnr}\")\n",
    "\n",
    "eval_network = ValidateCell(kernel_generation_net, upscale_net, downsampler_net, quant, scale, scale)\n",
    "val_dataloader = build_dataset(DIV2KHR(args.image_path, \"valid\"),\n",
    "                               batch_size=1,\n",
    "                               repeat_num=1,\n",
    "                               shuffle=False,\n",
    "                               num_parallel_workers=args.workers)\n",
    "\n",
    "cb_savecheckpoint = SaveCheckpoint(eval_network, val_dataloader, scale, args.checkpoint_path, args.eval_proid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start training..\n"
     ]
    }
   ],
   "source": [
    "from mindspore.train.callback import LossMonitor\n",
    "\n",
    "training = False\n",
    "if training:\n",
    "    print(\"start training..\")\n",
    "    model.train(args.end_epoch, train_dataloader, callbacks=[LossMonitor(), cb_savecheckpoint], dataset_sink_mode=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 模型评估\n",
    "\n",
    "模型评估采用\"Set5\", \"BSDS100\", \"Set14\", \"Urban100\", \"DIV2KHR\",将解压后的数据集放到./datasets/下，文件目录如下所示：\n",
    "\n",
    "```text\n",
    "\n",
    "        └── datasets\n",
    "             ├── Set5\n",
    "             |    ├── baby.png\n",
    "             |    ├── bird.png\n",
    "             |    ├── ...\n",
    "             ├── Set14\n",
    "             |    ├── baboon.png\n",
    "             |    ├── barbara.png\n",
    "             |    ├── ...\n",
    "             ├── BSDS100\n",
    "             |    ├── 101085.png\n",
    "             |    ├── 101087.png\n",
    "             |    ├── ...\n",
    "             ├── Urban100\n",
    "             |    ├── img_001.png\n",
    "             |    ├── img_002.png\n",
    "             |    ├── ...\n",
    "             └── DIV2K\n",
    "                    ├── DIV2K_train_HR\n",
    "                    |    ├── 0001.png\n",
    "                    |    ├── 0002.png\n",
    "                    |    ├── ...\n",
    "                    ├── DIV2K_valid_HR\n",
    "                    |    ├── 000801.png\n",
    "                    |    ├── 000802.png\n",
    "                    |    ├── ...\n",
    "\n",
    "```\n",
    "\n",
    "权重文件存放在./checkpoint下，目录如下：\n",
    "\n",
    "```text\n",
    "\n",
    "        └── checkpoint\n",
    "             ├── 2x\n",
    "             |    ├── kgn.ckpt\n",
    "             |    ├── usn.ckpt\n",
    "             └── 4x\n",
    "                  ├── kgn.ckpt\n",
    "                  └── usn.ckpt\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validating... Set5.Downscaling x4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5/5 [00:08<00:00,  1.73s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For 'Set5', save results at ./exp_res/Set5\n",
      "Mean PSNR: 34.17\n",
      "Mean SSIM: 0.9196\n",
      "==============================\n",
      "Validating... BSDS100.Downscaling x4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:47<00:00,  2.11it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For 'BSDS100', save results at ./exp_res/BSDS100\n",
      "Mean PSNR: 29.49\n",
      "Mean SSIM: 0.8092\n",
      "==============================\n",
      "Validating... Set14.Downscaling x4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 14/14 [00:20<00:00,  1.46s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For 'Set14', save results at ./exp_res/Set14\n",
      "Mean PSNR: 30.61\n",
      "Mean SSIM: 0.8427\n",
      "==============================\n",
      "Validating... Urban100.Downscaling x4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [04:20<00:00,  2.61s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For 'Urban100', save results at ./exp_res/Urban100\n",
      "Mean PSNR: 29.31\n",
      "Mean SSIM: 0.8704\n",
      "==============================\n",
      "Validating... DIV2KHR.Downscaling x4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [14:30<00:00,  8.71s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For 'DIV2KHR', save results at ./exp_res/DIV2KHR\n",
      "Mean PSNR: 32.68\n",
      "Mean SSIM: 0.8871\n",
      "==============================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "from mindspore import load_checkpoint, load_param_into_net, context\n",
    "from src.utils.metric import compute_psnr_ssim, ValidateCell\n",
    "from src.process_dataset.dataset import Set5Test\n",
    "\n",
    "\n",
    "kernel_generation_net = DSN(k_size=kernel_size, scale=scale)\n",
    "upscale_net = EDSR(32, 256, scale=scale)\n",
    "\n",
    "#load checkpoint\n",
    "\n",
    "kgn_dict = load_checkpoint(os.path.join(args.checkpoint_path, f\"{scale}x\", \"kgn.ckpt\"))\n",
    "usn_dict = load_checkpoint(os.path.join(args.checkpoint_path, f\"{scale}x\", \"usn.ckpt\"))\n",
    "load_param_into_net(kernel_generation_net, kgn_dict, strict_load=True)\n",
    "load_param_into_net(upscale_net, usn_dict, strict_load=True)\n",
    "kernel_generation_net.set_train(False)\n",
    "upscale_net.set_train(False)\n",
    "downsampler_net.set_train(False)\n",
    "quant.set_train(False)\n",
    "valid_net = ValidateCell(kernel_generation_net, upscale_net, downsampler_net, quant, scale, scale)\n",
    "\n",
    "#read data\n",
    "if not os.path.exists(args.output_dir):\n",
    "    os.makedirs(args.output_dir)\n",
    "\n",
    "target_dataset = [\"Set5\", \"BSDS100\", \"Set14\", \"Urban100\", \"DIV2KHR\"]\n",
    "\n",
    "for data_type in target_dataset:\n",
    "    if data_type == \"DIV2KHR\":\n",
    "        val_dataloader = build_dataset(DIV2KHR(\"./datasets/DIV2K\", \"valid\"), 1, 1, False) # 由于图片大小不一致，batch_size 设置为1\n",
    "    else:\n",
    "        val_dataloader = build_dataset(Set5Test(\"./datasets/\", data_type), 1, 1, False)\n",
    "\n",
    "    psnr_list = list()\n",
    "    ssim_list = list()\n",
    "    save_dir = os.path.join(args.output_dir, data_type)\n",
    "    if not os.path.exists(save_dir):\n",
    "        os.makedirs(save_dir)\n",
    "\n",
    "    print(f\"Validating... {data_type}.Downscaling x{scale}\")\n",
    "    for i, data in enumerate(tqdm(val_dataloader.create_dict_iterator(), total=val_dataloader.get_dataset_size())):\n",
    "        img = data['image']\n",
    "        downscaled_img, reconstructed_img = valid_net(img)\n",
    "        psnr, ssim = compute_psnr_ssim(img, downscaled_img, reconstructed_img, i, save_dir, scale, True)\n",
    "        psnr_list.append(psnr)\n",
    "        ssim_list.append(ssim)\n",
    "\n",
    "    print(f\"For \\'{data_type}\\', save results at {save_dir}\")\n",
    "    print('Mean PSNR: {0:.2f}'.format(np.mean(psnr_list)))\n",
    "    print('Mean SSIM: {0:.4f}'.format(np.mean(ssim_list)))\n",
    "    print(\"=\"*30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 结果对比\n",
    "\n",
    "与论文中的结果对比，误差均在3%以内\n",
    "![show_images](images/res.jpg)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "11c3c5397e9acf866e4a0f989d4b26c65610d75cb1253cd2a96c502d16f5ce98"
  },
  "kernelspec": {
   "display_name": "Python 3.9.12 ('ms')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
